# -*- coding: utf-8 -*-
"""AppliedMathematicsLab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c_wZbCmi47fQr0ishBmZbTz3Cq5FegM1

# Applied Mathematics Lab

Name: Lakshya Shrivastav

Roll: 199

Batch: C3

PRN: 202401040338
"""

!pip install scipy

#@title Scalar Multiplication
def scalar_multiply(matrix, scalar):
  result = []
  for row in matrix:
    result.append([scalar * x for x in row])
  return result

A = [[1,2],
     [3,4]]
scalar_multiply(A, 3)

#@title Eigenvalue and eigenvector
def eigen_2x2(m):
  a, b, c, d = m[0][0], m[0][1], m[1][0], m[1][1]

  determinant = a*d - b*c
  trace = a + d
  # print(a, b, c, d, trace, determinant)
  discriminant = trace**2 - 4*determinant

  if discriminant > 0:
    lambda1 = (trace + discriminant**(0.5)) / 2
    lambda2 = (trace - discriminant**(0.5)) / 2
  elif discriminant == 0:
    lambda1 = lambda2 = trace / 2
  else:
    print("Complex eigenvalues not supported")
    return

  def find_vector(l):
    v1 = -b / (a - l) if a - l != 0 else 1
    return [v1, 1]

  eigenvector1 = find_vector(lambda1)
  eigenvector2 = find_vector(lambda2)

  return [lambda1, lambda2], [eigenvector1, eigenvector2]

A = [[2,1],[-1,0]]
eigenvalues, eigenvectors = eigen_2x2(A)
print("Eigenvalues:", eigenvalues)
print("Eigenvectors:", eigenvectors)

#@title Gaussian ELimination

def upper_triangular(A, b):
  n = len(A)

  for i in range(n):
    pivot = A[i][i]
    for j in range(i, n):
      A[i][j] /= pivot
    b[i] /= pivot

    for k in range(i+1, n):
      factor = A[k][i]
      for j in range(i, n):
        A[k][j] -= factor * A[i][j]
      b[k] -= factor * b[i]


def gaussian_elimination(A, b):
  upper_triangular(A, b)
  n = len(A)
  x = [0] * n
  for i in range(n-1, -1, -1):
    x[i] = b[i]
    for j in range(i+1, n):
      x[i] -= A[i][j] * x[j]
  return x

A = [[1, 2],
     [3, 4]]
b = [1, 5]

gaussian_elimination(A, b)

#@title Probability Distribution

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

n=10
p=0.5
x_binomial = np.arange(0, n+1)
pmf_binomial = stats.binom.pmf(x_binomial, n, p)

mu = 0
sigma = 1
range = 2
x_normal = np.linspace(-range, range, 100)
pdf_normal = stats.norm.pdf(x_normal, mu, sigma)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.stem(x_binomial, pmf_binomial, basefmt=" ")
plt.title("Binomial Distribution: n=10, p=0.5")
plt.xlabel("Number of successes")
plt.ylabel("Probability")
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(x_normal, pdf_normal)
plt.title("Normal Distribution: mu=0, sigma=1")
plt.xlabel("Value")
plt.ylabel("Probability density")
plt.grid()

plt.tight_layout()

#@title Bivariate Discrete Random variable

import numpy as np
import matplotlib.pyplot as plt

die_outcomes = [1, 2, 3, 4, 5, 6]

sample_space = [(i,j) for i in die_outcomes for j in die_outcomes]

joint_pmf = {}
for (x,y) in sample_space:
  joint_pmf[(x,y)] = 1/36

pmf_matrix = np.zeros((6,6))
for i in range(len(die_outcomes)):
  for j in range(len(die_outcomes)):
    pmf_matrix[i,j] = joint_pmf[(i+1, j+1)]

plt.figure(figsize=(7,6))
plt.imshow(pmf_matrix, cmap="Blues", interpolation="nearest")


for i in range(len(die_outcomes)):
  for j in range(len(die_outcomes)):
    plt.text(i, j, f"{pmf_matrix[i, j]:.3f}", ha="center", va="center", color="black")

plt.title("Joint Probability Mass Function (2 Die)")
plt.xlabel("Y (Die 2)")
plt.ylabel("X (DIe 1)")
plt.colorbar(label="Probability")
plt.xticks(np.arange(6), die_outcomes)
plt.yticks(np.arange(6), reversed(die_outcomes))
plt.show()


# Marginal probability
marginal_X = {x: sum(pmf_matrix[x-1]) for x in die_outcomes}
marginal_Y = {y: sum(pmf_matrix[:,y-1]) for y in die_outcomes}
print(marginal_X)
print(marginal_Y)

def P(x,y):
  if type(x) != list:
    x = [x]
  if type(y) != list:
    y = [y]
  return sum(pmf_matrix[x[0]:x[-1]+1,y[0]:y[-1]+1].flatten())

print(P(x=[1,2], y=[1,2]))
# #P(X=1,Y=2)
# print(marginal_X[1] * marginal_Y[2])

# #P(X=1,Y<3)
# print(marginal_X[1] * sum([marginal_Y[i] for i in [1,2]]))

# pmf_matrix[1:3, 2:4]

#@title Central Limit Theorem

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from scipy.stats import binom

population_size = 100000
# population = np.random.uniform(0,1,population_size)
# population = binom.pmf(np.arange(0, population_size+1), population_size, 0.9)
population = np.random.binomial(n=10, p=0.5, size=population_size)
print(population)
sample_size = 30
num_samples = 1000

sample_means = np.empty(num_samples)
for i in range(num_samples):
  sample = np.random.choice(population, sample_size)
  sample_means[i] = np.mean(sample)


plt.figure(figsize=(10, 5))
plt.hist(sample_means, bins=30, density=True, alpha=0.6, color="skyblue", label="Sample Means (CLT)")
mean = np.mean(sample_means)
std = np.std(sample_means)

#@title t-distribution and F-distribution

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t, f

df_t = 10
x_t = np.linspace(-5, 5, 500)
pdf_t = t.pdf(x_t, df_t)

dfn, dfd = .5, 20
x_f = np.linspace(0, 5, 500)
pdf_f = f.pdf(x_f, dfn, dfd)


plt.figure(figsize=(12, 5))

plt.subplot(1,2,1)
plt.plot(x_t, pdf_t, "b", label=f"t-distribution (df={df_t})")
plt.title("t-distribution")
plt.xlabel("X")
plt.ylabel("Probability density")
plt.legend()
plt.grid(True)

plt.subplot(1,2,2)
plt.plot(x_f, pdf_f, "r", label=f"f-distribution (dfn={dfn}, dfd={dfd})")
plt.title("f-distribution")
plt.xlabel("X")
plt.ylabel("Probability density")
plt.legend()
plt.grid(True)

#@title Z test

import numpy as np
from scipy.stats import norm

def z_test_single_mean(sample, mu0, sigma=None, alpha=0.05, alternative="two-sided"):
  sample = np.array(sample)
  n = len(sample)
  x_bar = sample.mean()

  if sigma is None:
    sigma = np.std(sample, ddof=1)

  z_stat = (x_bar - mu0) / (sigma / np.sqrt(n))

  match(alternative):
    case "two-sided":
      p_value = 2 * (1-norm.cdf(abs(z_stat)))
    case "greater":
      p_value = 1 - norm.cdf(z_stat)
    case "less":
      p_value = norm.cdf(z_stat)
    case _:
      raise ValueError("alternative must be 'two-sided', 'less' or 'greater'")

  reject = p_value > alpha

  return {
      "sample-mean": x_bar,
      "sigma": sigma,
      "hypothesized-mean": mu0,
      "Z-stat": z_stat,
      "p-value": p_value,
      "conclusion": "reject H0" if reject else "fail to reject H0"
  }

population = np.random.normal(loc=50, scale=1, size=1000)
# sample = [52, 50, 50, 49, 51, 50, 50, 52, 49]
sample = np.random.choice(population, size=30)
mu0 = 48
print(np.std(sample, ddof=1))
z_test_single_mean(sample, mu0, alpha=0.05, sigma=1)
# 1.7007115230821842067

#@title t-test for Difference of Means

import numpy as np
from scipy.stats import t

def t_test_two_means(sample1, sample2, alpha=0.05, alternative="two-sided", equal_var=True):
    """
    Perform t-test for difference of means between two samples
    """
    sample1 = np.array(sample1)
    sample2 = np.array(sample2)

    n1 = len(sample1)
    n2 = len(sample2)
    x_bar1 = sample1.mean()
    x_bar2 = sample2.mean()
    s1 = np.std(sample1, ddof=1)
    s2 = np.std(sample2, ddof=1)

    # Calculate pooled standard deviation if equal variances assumed
    if equal_var:
        s_pooled = np.sqrt(((n1-1)*s1**2 + (n2-1)*s2**2) / (n1 + n2 - 2))
        std_error = s_pooled * np.sqrt(1/n1 + 1/n2)
        df = n1 + n2 - 2
    else:
        # Welch's t-test
        std_error = np.sqrt(s1**2/n1 + s2**2/n2)
        df = (s1**2/n1 + s2**2/n2)**2 / ((s1**2/n1)**2/(n1-1) + (s2**2/n2)**2/(n2-1))

    t_stat = (x_bar1 - x_bar2) / std_error

    match alternative:
        case "two-sided":
            p_value = 2 * (1 - t.cdf(abs(t_stat), df))
        case "greater":
            p_value = 1 - t.cdf(t_stat, df)
        case "less":
            p_value = t.cdf(t_stat, df)
        case _:
            raise ValueError("alternative must be 'two-sided', 'less' or 'greater'")

    reject = p_value < alpha

    return {
        "sample1-mean": x_bar1,
        "sample2-mean": x_bar2,
        "sample1-std": s1,
        "sample2-std": s2,
        "mean-difference": x_bar1 - x_bar2,
        "t-stat": t_stat,
        "degrees-of-freedom": df,
        "p-value": p_value,
        "conclusion": "reject H0" if reject else "fail to reject H0"
    }

# Example usage
population1 = np.random.normal(loc=50, scale=2, size=1000)
population2 = np.random.normal(loc=52, scale=2, size=1000)

sample1 = np.random.choice(population1, size=15)
sample2 = np.random.choice(population2, size=15)

print("t-test for Difference of Means:")
result = t_test_two_means(sample1, sample2, alpha=0.05, alternative="two-sided")
for key, value in result.items():
    print(f"{key}: {value}")

#@title Test for Single Proportion

import numpy as np
from scipy.stats import norm

def z_test_single_proportion(sample, p0, alpha=0.05, alternative="two-sided"):
    """
    Perform z-test for single proportion (large sample)
    """
    sample = np.array(sample)
    n = len(sample)

    # Count successes (assuming binary data: 1=success, 0=failure)
    if np.all((sample == 0) | (sample == 1)):
        x = np.sum(sample)
    else:
        # If not binary, assume we're given the count directly
        x = sample[0] if len(sample) == 1 else np.sum(sample)

    p_hat = x / n

    # Check large sample condition
    if n * p0 < 5 or n * (1 - p0) < 5:
        print("Warning: Sample size may be too small for normal approximation")

    # Standard error under H0
    std_error = np.sqrt(p0 * (1 - p0) / n)

    z_stat = (p_hat - p0) / std_error

    match alternative:
        case "two-sided":
            p_value = 2 * (1 - norm.cdf(abs(z_stat)))
        case "greater":
            p_value = 1 - norm.cdf(z_stat)
        case "less":
            p_value = norm.cdf(z_stat)
        case _:
            raise ValueError("alternative must be 'two-sided', 'less' or 'greater'")

    reject = p_value < alpha

    return {
        "sample-proportion": p_hat,
        "successes": x,
        "sample-size": n,
        "hypothesized-proportion": p0,
        "Z-stat": z_stat,
        "p-value": p_value,
        "conclusion": "reject H0" if reject else "fail to reject H0"
    }

# Example usage
# Generate binary data (1 = success, 0 = failure)
np.random.seed(42)
sample_prop = np.random.binomial(1, 0.6, size=100)  # 100 trials with true p=0.6
p0 = 0.5  # hypothesized proportion

print("\nTest for Single Proportion:")
result = z_test_single_proportion(sample_prop, p0, alpha=0.05, alternative="two-sided")
for key, value in result.items():
    print(f"{key}: {value}")

#@title F-test for Comparison of Variances

import numpy as np
from scipy.stats import f

def f_test_two_variances(sample1, sample2, alpha=0.05, alternative="two-sided"):
    """
    Perform F-test for comparison of two variances
    """
    sample1 = np.array(sample1)
    sample2 = np.array(sample2)

    n1 = len(sample1)
    n2 = len(sample2)

    var1 = np.var(sample1, ddof=1)
    var2 = np.var(sample2, ddof=1)

    # F-statistic (always put larger variance in numerator)
    if var1 >= var2:
        f_stat = var1 / var2
        df1 = n1 - 1
        df2 = n2 - 1
    else:
        f_stat = var2 / var1
        df1 = n2 - 1
        df2 = n1 - 1

    match alternative:
        case "two-sided":
            p_value = 2 * min(1 - f.cdf(f_stat, df1, df2), f.cdf(f_stat, df1, df2))
        case "greater":
            p_value = 1 - f.cdf(f_stat, df1, df2)
        case "less":
            p_value = f.cdf(f_stat, df1, df2)
        case _:
            raise ValueError("alternative must be 'two-sided', 'less' or 'greater'")

    reject = p_value < alpha

    return {
        "sample1-variance": var1,
        "sample2-variance": var2,
        "variance-ratio": var1/var2,
        "F-stat": f_stat,
        "df-numerator": df1,
        "df-denominator": df2,
        "p-value": p_value,
        "conclusion": "reject H0" if reject else "fail to reject H0"
    }

# Example usage
np.random.seed(42)
population1 = np.random.normal(loc=50, scale=3, size=1000)  # std=3
population2 = np.random.normal(loc=50, scale=2, size=1000)  # std=2

sample1 = np.random.choice(population1, size=12)
sample2 = np.random.choice(population2, size=12)

print("\nF-test for Comparison of Variances:")
result = f_test_two_variances(sample1, sample2, alpha=0.05, alternative="two-sided")
for key, value in result.items():
    print(f"{key}: {value}")

#@title Markov Chain

import numpy as np
import matplotlib.pyplot as plt

def validate_transition_matrix(P):
  P = np.array(P, dtype=float)
  if P.ndim != 2 or P.shape[0] != P.shape[1]:
    raise ValueError("Transition matrixmust be square (n x n).")
  if not np.allclose(P.sum(axis=1), 1):
    raise ValueError("Each row of transition matrix must sum to 1.")
  if np.any(P < 0):
    raise ValueError("Transition matrix cannot have negative entries.")
  return P

def n_step_transition(P, n):
  P = validate_transition_matrix(P)
  return np.linalg.matrix_power(P, n)

P = np.array([
    [0.5 , 0.3 , 0.2],
    [0.2 , 0.4 , 0.4],
    [0.0 , 0.6 , 0.4],
])

steps = [1, 2, 5, 10, 20]

results = {}
for n in steps:
  Pn = n_step_transition(P, n)
  results[n] = Pn
  print(f"\n{n}-step trnsition_probability matrix:\n{Pn}")

max_n = 20
state_names = ["S" + str(i) for i in range(0, P.shape[0])]
evolution = [n_step_transition(P, n)[0] for n in range(1, max_n+1)]
print("Evolution = ", evolution)
evolution = np.array(evolution)

plt.figure(figsize=(8, 5))
for i in range(P.shape[0]):
  plt.plot(range(1, max_n+1), evolution[:, i], marker="o", label=f"P(S0->{state_names[i]})")
plt.xlabel("Steps (n)")
plt.ylabel("Transition Probability")
plt.title("n-step Transition Probability from State S0")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

#@title Markov chain
import numpy as np

def simulate_first_order_markov(transition_matrix, initial_state, n_steps):
    n_states = transition_matrix.shape[0]
    states = [initial_state]
    current_state = initial_state

    for _ in range(n_steps):
        next_state = np.random.choice(n_states, p=transition_matrix[current_state])
        states.append(next_state)
        current_state = next_state

    return states

def calculate_n_step_transition(transition_matrix, n):
    return transition_matrix**n

def calculate_steady_state(transition_matrix, max_iter=1000, tol=1e-8):
    n_states = transition_matrix.shape[0]
    pi = np.ones(n_states) / n_states

    for _ in range(max_iter):
        pi_new = pi @ transition_matrix
        if np.linalg.norm(pi_new - pi) < tol:
            return pi_new
        pi = pi_new

    return pi

def simulate_second_order_markov(transition_tensor, initial_states, n_steps):
    states = initial_states.copy()

    for _ in range(n_steps):
        prev_state = states[-2]
        current_state = states[-1]
        next_state = np.random.choice(transition_tensor.shape[2], p=transition_tensor[prev_state, current_state])
        states.append(next_state)

    return states

# Example usage for First-Order Markov Process
transition_matrix = np.array([
    [0.7, 0.2, 0.1],
    [0.3, 0.4, 0.3],
    [0.2, 0.3, 0.5]
])

initial_state = 0
n_steps = 100

states = simulate_first_order_markov(transition_matrix, initial_state, n_steps)
print(states)


# Example usage for n-Step Transition Probability
n = 5
n_step_matrix = calculate_n_step_transition(transition_matrix, n)
print(n_step_matrix)


# Example usage for Steady State Distribution
steady_state = calculate_steady_state(transition_matrix)
print(steady_state)


# Example usage for Second-Order Markov Process
transition_tensor = np.array([
    [
        [0.6, 0.3, 0.1],
        [0.2, 0.5, 0.3],
        [0.1, 0.4, 0.5]
    ],
    [
        [0.5, 0.3, 0.2],
        [0.3, 0.4, 0.3],
        [0.2, 0.3, 0.5]
    ],
    [
        [0.4, 0.4, 0.2],
        [0.3, 0.3, 0.4],
        [0.1, 0.2, 0.7]
    ]
])

initial_states = [0, 1]
n_steps = 100

states_second_order = simulate_second_order_markov(transition_tensor, initial_states, n_steps)
print(states_second_order)

#@title Correlation and Regression
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression

def karl_pearson_correlation(x, y):
    n = len(x)
    sum_x = np.sum(x)
    sum_y = np.sum(y)
    sum_xy = np.sum(x * y)
    sum_x2 = np.sum(x**2)
    sum_y2 = np.sum(y**2)
    numerator = n * sum_xy - sum_x * sum_y
    denominator = np.sqrt((n * sum_x2 - sum_x**2) * (n * sum_y2 - sum_y**2))
    return numerator / denominator

def spearman_rank_correlation(x, y):
    rank_x = stats.rankdata(x)
    rank_y = stats.rankdata(y)
    n = len(x)
    d = rank_x - rank_y
    d_squared = np.sum(d**2)
    return 1 - (6 * d_squared) / (n * (n**2 - 1))

def simple_linear_regression(x, y):
    x = np.array(x).reshape(-1, 1)
    y = np.array(y)
    model = LinearRegression()
    model.fit(x, y)
    slope = model.coef_[0]
    intercept = model.intercept_
    r_squared = model.score(x, y)
    return slope, intercept, r_squared

def multiple_linear_regression(X, y):
    model = LinearRegression()
    model.fit(X, y)
    coefficients = model.coef_
    intercept = model.intercept_
    r_squared = model.score(X, y)
    return coefficients, intercept, r_squared

np.random.seed(42)
x_pearson = np.random.normal(0, 1, 100)
y_pearson = 2 * x_pearson + np.random.normal(0, 0.5, 100)
pearson_corr = karl_pearson_correlation(x_pearson, y_pearson)

x_spearman = np.array([1, 2, 3, 4, 5])
y_spearman = np.array([2, 4, 1, 5, 3])
spearman_corr = spearman_rank_correlation(x_spearman, y_spearman)

x_simple = np.random.normal(0, 1, 100).reshape(-1, 1)
y_simple = 3 * x_simple.flatten() + 2 + np.random.normal(0, 0.5, 100)
slope, intercept, r2_simple = simple_linear_regression(x_simple, y_simple)

X_multiple, y_multiple = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)
coefficients, intercept_multiple, r2_multiple = multiple_linear_regression(X_multiple, y_multiple)

print(pearson_corr)
print(spearman_corr)
print(slope, intercept, r2_simple)
print(coefficients, intercept_multiple, r2_multiple)

#@title Lagrange Interpolation

import numpy as np
import matplotlib.pyplot as plt
import math

def f(x):
  return math.sin(x)

x = np.array((1, 2, 3, 4, 5))
y = np.array([f(i) for i in x])

def lagrange_interpolation(x, y, x_val):
  n = len(x)
  result = 0

  for i in range(n):
    term = y[i]
    for j in range(n):
      if i != j:
        term *= (x_val - x[j]) / (x[i] - x[j])
    result += term
  return result

x_new = 2.5
y_new = lagrange_interpolation(x, y, x_new)

print(x_new, y_new)


x_range = np.linspace(min(x), max(x), 100)
y_range = [lagrange_interpolation(x, y, xi) for xi in x_range]

error = [lagrange_interpolation(x,y,xi) - f(xi) for xi in x_range]
print("Mean Squared Error:", sum([i**2 for i in error])/len(error))

plt.figure()
plt.scatter(x, y, color="red", label="Data Points")
plt.plot(x_range, y_range, color="blue", label = "Lagrange Interpolation")
plt.title("Lagrange Interpolation")
plt.grid(True)
plt.show()

#@title Newton Interpolation

def divided_diff(x, y):
  n = len(y)
  table = np.zeros((n,n))

  for i in range(n):
    table[i][0] = y[i]

  # Calculate divided differences
  for j in range(1, n):
    for i in range(n - j):
      table[i][j] = (table[i+1][j-1] - table[i][j-1]) / (x[i+j] - x[i])

  return table

def newton_interpolation(x, y, value):
  n = len(x)
  table = divided_diff(x,y)
  result = table[0][0]

  product_term = 1
  for i in range(1, n):
    product_term *= (value - x[i-1])
    result += table[0][i] * product_term

  return result


def f(x):
  return math.sin(x)

x = np.array((1, 2, 3, 4, 5))
y = np.array([f(i) for i in x])

x_val = 2.5
y_val = newton_interpolation(x, y, x_val)
print(x_val, y_val)

x_range = np.linspace(min(x), max(x), 100)
y_range = [newton_interpolation(x, y, xi) for xi in x_range]

error = [newton_interpolation(x,y,xi) - f(xi) for xi in x_range]
print("Mean Squared Error:", sum([i**2 for i in error])/len(error))

plt.figure()
plt.scatter(x, y, color="red", label="Data Points")
plt.plot(x_range, y_range, color="blue", label = "Newton Interpolation")
plt.title("Newton Interpolation")
plt.grid(True)
plt.show()

#@title Gauss Elimination
import numpy as np

def gauss_elimination(A, b):
    n = len(b)
    Ab = np.hstack([A.astype(float), b.reshape(-1, 1).astype(float)])

    for i in range(n):
        max_row = np.argmax(np.abs(Ab[i:, i])) + i
        Ab[[i, max_row]] = Ab[[max_row, i]]

        Ab[i] = Ab[i] / Ab[i, i]

        for j in range(i + 1, n):
            Ab[j] = Ab[j] - Ab[j, i] * Ab[i]

    x = np.zeros(n)
    for i in range(n - 1, -1, -1):
        x[i] = Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])

    return x

A_gauss = np.array([[2, 1, -1],
                    [-3, -1, 2],
                    [-2, 1, 2]])
b_gauss = np.array([8, -11, -3])

gauss_solution = gauss_elimination(A_gauss, b_gauss)
print(gauss_solution)

#@title LU decomposition
import numpy as np

def lu_decomposition(A):
    n = A.shape[0]
    L = np.eye(n)
    U = A.astype(float)

    for i in range(n):
        for j in range(i + 1, n):
            L[j, i] = U[j, i] / U[i, i]
            U[j, i:] = U[j, i:] - L[j, i] * U[i, i:]

    return L, U

def solve_lu(L, U, b):
    n = len(b)
    y = np.zeros(n)
    x = np.zeros(n)

    for i in range(n):
        y[i] = b[i] - np.dot(L[i, :i], y[:i])

    for i in range(n - 1, -1, -1):
        x[i] = (y[i] - np.dot(U[i, i + 1:], x[i + 1:])) / U[i, i]

    return x

A_lu = np.array([[2, 1, -1],
                 [-3, -1, 2],
                 [-2, 1, 2]])
b_lu = np.array([8, -11, -3])

L, U = lu_decomposition(A_lu)
lu_solution = solve_lu(L, U, b_lu)

print(lu_solution)

#@title Simple Linear Regression without libs
import numpy as np

def simple_linear_regression(x, y):
    n = len(x)
    x_mean = np.mean(x)
    y_mean = np.mean(y)

    numerator = np.sum((x - x_mean) * (y - y_mean))
    denominator = np.sum((x - x_mean) ** 2)

    slope = numerator / denominator
    intercept = y_mean - slope * x_mean

    y_pred = slope * x + intercept
    r_squared = 1 - (np.sum((y - y_pred) ** 2) / np.sum((y - y_mean) ** 2))

    return slope, intercept, r_squared

x_simple = np.array([1, 2, 3, 4, 5])
y_simple = np.array([2, 4, 5, 4, 5])

slope, intercept, r2 = simple_linear_regression(x_simple, y_simple)
print(slope, intercept, r2)

#@title Multiple Linear Regression without libs
import numpy as np

def multiple_linear_regression(x1, x2, y):
    n = len(y)
    X = np.column_stack([np.ones(n), x1, x2])

    XT = X.T
    XTX = XT @ X
    XTX_inv = np.linalg.inv(XTX)
    XTY = XT @ y

    coefficients = XTX_inv @ XTY
    intercept = coefficients[0]
    slope1 = coefficients[1]
    slope2 = coefficients[2]

    y_pred = intercept + slope1 * x1 + slope2 * x2
    y_mean = np.mean(y)
    r_squared = 1 - (np.sum((y - y_pred) ** 2) / np.sum((y - y_mean) ** 2))

    return intercept, slope1, slope2, r_squared

x1_multiple = np.array([1, 2, 3, 4, 5])
x2_multiple = np.array([2, 3, 2, 4, 3])
y_multiple = np.array([3, 5, 6, 8, 7])

intercept, slope1, slope2, r2 = multiple_linear_regression(x1_multiple, x2_multiple, y_multiple)
print(intercept, slope1, slope2, r2)